{
 "metadata": {
  "name": "Entity_Finder"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Named-entity recognition</h2>\n",
      "<p>In <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" title=\"natural language processing - wikipedia\" target=\"_blank\">natural language processing</a>, <a href=\"http://en.wikipedia.org/wiki/Named-entity_recognition\" title=\"named entity recognition - wikipedia\" target=\"_blank\">entity recognition</a> problems are those in which the principal task is to identify irreducible elements within text like people, places, locations, products, companies, measurements (e.g. dollars, miles, percentages, etc.) and more.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>The Dataset</h2>\n",
      "<p>As an example, we'll look at text corpuses from <a href=\"http://www.tv.com\" title=\"tv.com\" target=\"_blank\">tv.com</a> The dataset consists of user-submitted recaps of Law and Order episodes..</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('./episodes_and_recaps.txt', sep='|')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'nrow: %d' % len(df)\n",
      "print 'ncol: %d' % len(df.columns)\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "nrow: 775\n",
        "ncol: 13\n"
       ]
      },
      {
       "html": [
        "<pre>\n",
        "&ltclass 'pandas.core.frame.DataFrame'&gt\n",
        "Int64Index: 775 entries, 0 to 774\n",
        "Data columns (total 13 columns):\n",
        "directed_by            775  non-null values\n",
        "no_in_season           775  non-null values\n",
        "no_in_series           775  non-null values\n",
        "original_air_date      775  non-null values\n",
        "production_code        680  non-null values\n",
        "title                  775  non-null values\n",
        "us_viewers_millions    428  non-null values\n",
        "written_by             775  non-null values\n",
        "nth_season             775  non-null values\n",
        "show                   775  non-null values\n",
        "corpus_url             775  non-null values\n",
        "source                 775  non-null values\n",
        "corpus                 167  non-null values\n",
        "dtypes: float64(1), int64(3), object(9)\n",
        "</pre>"
       ],
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 775 entries, 0 to 774\n",
        "Data columns (total 13 columns):\n",
        "directed_by            775  non-null values\n",
        "no_in_season           775  non-null values\n",
        "no_in_series           775  non-null values\n",
        "original_air_date      775  non-null values\n",
        "production_code        680  non-null values\n",
        "title                  775  non-null values\n",
        "us_viewers_millions    428  non-null values\n",
        "written_by             775  non-null values\n",
        "nth_season             775  non-null values\n",
        "show                   775  non-null values\n",
        "corpus_url             775  non-null values\n",
        "source                 775  non-null values\n",
        "corpus                 167  non-null values\n",
        "dtypes: float64(1), int64(3), object(9)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Number of episodes in the Law and Order franchise'\n",
      "df.groupby(['show']).size()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of episodes in the Law and Order franchise\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "show\n",
        "original    456\n",
        "svu         319\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Because these are submitted by users, not every episode will have a recap / corpus.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = df[df.corpus.notnull()].corpus.values[9]\n",
      "print text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "After a woman called Angela Jarrell is discovered dead in front of a bakery, her purse, which the detectives come across two blocks away, is found to contain a bag of ecstasy. From the victim's stomach contents, they place her in a bar, where a waitress finds a credit card slip from a young man who paid for her drinks. This man turns out to be innocent, but he gives the detectives the number of Jarrell's (stolen) cell phone, which leads them to a Mr. Daltry, who claims he was trying to purchase \n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Extracting names of characters from the text</h2>\n",
      "<p>Suppose we wanted to know which neighborhoods were most often the scene of the crime in Law and Order or SVU. We could read through all the episode recaps submitted by users and look for neighborhoods mentioned. But there are 456 episodes spanning 20 seasons in the original Law and Order and 319+ episodes spanning 15 seasons. That's a lot of episode recaps to read through.</p>\n",
      "\n",
      "<h2>Using NLTK and Python</h2>\n",
      "<p>For this example, we'll write a script to extract the named entities from these episode recaps programatically.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Our general strategy will be to transform our unstructured text data into structured data. To accomplish this, we'll be using several utilities found in Python's <a href=\"\" title=\"nltk - natural language toolkit for python\" target=\"_blank\">NLTK</a> (Natural Language Toolkit) library, a package with lots of great functions and routines for tokenizing and learning text.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Tokenize text into sentences</h4>\n",
      "<p>First things first. We need to break the corpus into individual sentences. This can be done using NLTK's <code>sent_tokenize</code> function (<a href=\"http://nltk.googlecode.com/svn/trunk/doc/api/nltk.tokenize-module.html#sent_tokenize\" title=\"nltk sent_tokenize function\" target=\"_blank\">read more here</a>).</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(nltk.sent_tokenize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function sent_tokenize in module nltk.tokenize:\n",
        "\n",
        "sent_tokenize(text)\n",
        "    Return a sentence-tokenized copy of *text*,\n",
        "    using NLTK's recommended sentence tokenizer\n",
        "    (currently :class:`.PunktSentenceTokenizer`).\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>The <code>sent_tokenize</code> function is pretty cool. Notice that NLTK knows that the period in \"Mr. Daltry\" (sentence #1) isn't the end of a sentence? It'll also handle sentences that start w/ lowercase letters.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = nltk.sent_tokenize(text)\n",
      "print 'Original sentence\\n'\n",
      "print text[ text.index(\"This man turns out\"): text.index(\"Frances Partell\")+len(\"Frances Partell\")+1]\n",
      "print\n",
      "print 'tokenized\\n'\n",
      "for i, sent in enumerate(sentences[2:]):\n",
      "    print i, sent\n",
      "    print\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original sentence\n",
        "\n",
        "This man turns out to be innocent, but he gives the detectives the number of Jarrell's (stolen) cell phone, which leads them to a Mr. Daltry, who claims he was trying to purchase ecstasy from her and calls her an incompetent drug dealer. Daltry points the detectives to a drug dealer known as Taz, whose real name is Frances Partell.\n",
        "\n",
        "tokenized\n",
        "\n",
        "0 This man turns out to be innocent, but he gives the detectives the number of Jarrell's (stolen) cell phone, which leads them to a Mr. Daltry, who claims he was trying to purchase ecstasy from her and calls her an incompetent drug dealer.\n",
        "\n",
        "1 Daltry points the detectives to a drug dealer known as Taz, whose real name is Frances Partell.While the detectives can find nothing connecting Partell with Jarrell's murder, they discover from his former partner, Mr. Quintana, that he was involved in the murder of a bouncer at a club in the Bronx in 1998--for which another man, Tony Shaeffer, was sentenced after he boasted to his girl-friend of the murder.\n",
        "\n",
        "2 The detectives re-interview the witnesses of the bouncer murder and arrest Partell for it.Unfortunately, this arrest raises serious interoffice issues with the detectives, prosecutors, and DAs in the Bronx-the only reason the Manhattan office is allowed to prosecute the case in the first place is because the club is 488 yards from the county line.\n",
        "\n",
        "3 In spite of various witnesses raising doubts about the case, DA Robertson in the Bronx refuses to reconsider the case.\n",
        "\n",
        "4 The Manhattan DAs take the case to court nevertheless and manage to win an evidentiary hearing.\n",
        "\n",
        "5 Lewin asks McCoy and Carmichael to strike a deal with Partell in which he admits his guilt to get Shaeffer out of jail, but even after Partell's confession DA Robertson refuses to budge.\n",
        "\n",
        "6 McCoy takes the case to the Appellate Court and manages to win freedom for Shaeffer.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>As a rule, NLTK does a really good job of tokenization without a lot of fine tuning. If <em>you do</em> have some specific text that demands special behavior around tokenization, there are a lot of great options for adjusting and overriding the default behavior too.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Tokenize sentences into words</h4>\n",
      "<p>Next, we need to tokenize each sentence into its individual words.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
      "print \"First 20 words of the first sentence\\n\"\n",
      "print tokenized[0][:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First 20 words of the first sentence\n",
        "\n",
        "['After', 'a', 'woman', 'called', 'Angela', 'Jarrell', 'is', 'discovered', 'dead', 'in', 'front', 'of', 'a', 'bakery', ',', 'her', 'purse', ',', 'which', 'the']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Label Parts of Speech</h4>\n",
      "<p>Finally, we need to label each word with its part of speech. This will enable us to discern nouns (and proper nouns) from everything else later on.</p>\n",
      "<p>A lot can--and has--been said about <a href=\"http://en.wikipedia.org/wiki/Part-of-speech_tagging\" title=\"part of speech tagging - wikipedia\" target=\"_blank\">Part-of-speech tagging</a>. Since many of the details are outside the scope of this blog post, I'll go through some of the basics of POS tagging using NLTK and leave some cool references at the end for anybody interested to read more.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>NLTK's <code>pos_tag</code> function</h4>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>NLTK's <code>pos_tag</code> is NLTK's primary off-the-shelf tagger for parts of speech. It relies on the <a href=\"http://www.cis.upenn.edu/~treebank/\" title=\"Penn Treebank annotates the linguistic structure of natural language text.\" target=\"_blank\">Penn Treebank tagset</a> and encodes a list of tokens as tuples with shape <code>(tag, token)</code>.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(nltk.pos_tag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function pos_tag in module nltk.tag:\n",
        "\n",
        "pos_tag(tokens)\n",
        "    Use NLTK's currently recommended part of speech tagger to\n",
        "    tag the given list of tokens.\n",
        "    \n",
        "        >>> from nltk.tag import pos_tag\n",
        "        >>> from nltk.tokenize import word_tokenize\n",
        "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
        "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',\n",
        "        'VBZ'), (\"n't\", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),\n",
        "        ('.', '.')]\n",
        "    \n",
        "    :param tokens: Sequence of tokens to be tagged\n",
        "    :type tokens: list(str)\n",
        "    :return: The tagged tokens\n",
        "    :rtype: list(tuple(str, str))\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>If you don't have the Penn Treebank tagset installed, you can get it using NLTK's built-in downloader tool like so:</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import nltk\n",
      "# nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Take the following sentence for example:</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"Alan Shearer is the first player to score over a hundred Premier League goals.\"\n",
      "a_sentences = nltk.sent_tokenize(a)\n",
      "a_words     = [nltk.word_tokenize(sentence) for sentence in a_sentences]\n",
      "a_pos       = [nltk.pos_tag(sentence) for sentence in a_words]\n",
      "a_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[[('Alan', 'NNP'),\n",
        "  ('Shearer', 'NNP'),\n",
        "  ('is', 'VBZ'),\n",
        "  ('the', 'DT'),\n",
        "  ('first', 'JJ'),\n",
        "  ('player', 'NN'),\n",
        "  ('to', 'TO'),\n",
        "  ('score', 'VB'),\n",
        "  ('over', 'IN'),\n",
        "  ('a', 'DT'),\n",
        "  ('hundred', 'CD'),\n",
        "  ('Premier', 'NNP'),\n",
        "  ('League', 'NNP'),\n",
        "  ('goals', 'NNS'),\n",
        "  ('.', '.')]]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Take a look at the use of the word \"over\" in the above sentence. The <code>'IN'</code> tag in the tuple <code>('over', 'IN')</code> indicates that it's being used as a preposition in the phrase \"over a hundred.\"</p>\n",
      "<p>Conversely, </p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = \"Hank Mardukas was over-served at the bar last night.\"\n",
      "b_sentences = nltk.sent_tokenize(b)\n",
      "b_words     = [nltk.word_tokenize(sentence) for sentence in b_sentences]\n",
      "b_pos       = [nltk.pos_tag(sentence) for sentence in b_words]\n",
      "b_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[[('Hank', 'NNP'),\n",
        "  ('Mardukas', 'NNP'),\n",
        "  ('was', 'VBD'),\n",
        "  ('over-served', 'JJ'),\n",
        "  ('at', 'IN'),\n",
        "  ('the', 'DT'),\n",
        "  ('bar', 'NN'),\n",
        "  ('last', 'JJ'),\n",
        "  ('night', 'NN'),\n",
        "  ('.', '.')]]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>This time, \"over-served\" is tagged as <code>'JJ'</code> (adjective). NLTK knows that \"over\" is part of the attributive adjective phrase describing Hank and the potentially embarassing state he found himself in at the bar last night.</p>\n",
      "\n",
      "<p>We can apply this routine to our Law and Order episode recaps like so:</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_tags  = [nltk.pos_tag(sentence) for sentence in tokenized]\n",
      "print \"First 10 (word, parts of speech) in the first sentence\\n\"\n",
      "print pos_tags[0][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First 10 (word, parts of speech) in the first sentence\n",
        "\n",
        "[('After', 'IN'), ('a', 'DT'), ('woman', 'NN'), ('called', 'VBN'), ('Angela', 'NNP'), ('Jarrell', 'NNP'), ('is', 'VBZ'), ('discovered', 'VBN'), ('dead', 'JJ'), ('in', 'IN')]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Extracting the Entities</h2>\n",
      "<p>NLTK gives us some really powerful methods for isolating entities in text. One of the simplest and most powerful tools at our disposal is the <code>batch_ne_chunk</code> function which takes a list of tagged tokens and returns a list of named entity chunks.</p>\n",
      "\n",
      "<p>You can chunk sentences by passing sentences that have been tagged with parts-of-speech to <code>batch_ne_chunk</code>.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "named_entity_chunks =  nltk.batch_ne_chunk(pos_tags)\n",
      "print sentences[0]\n",
      "print named_entity_chunks[0][:9]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "After a woman called Angela Jarrell is discovered dead in front of a bakery, her purse, which the detectives come across two blocks away, is found to contain a bag of ecstasy.\n",
        "[('After', 'IN'), ('a', 'DT'), ('woman', 'NN'), ('called', 'VBN'), Tree('PERSON', [('Angela', 'NNP'), ('Jarrell', 'NNP')]), ('is', 'VBZ'), ('discovered', 'VBN'), ('dead', 'JJ'), ('in', 'IN')]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'List of tagged tokens'\n",
      "print [nltk.pos_tag(sentence) for sentence in tokenized][:1][0][:6]\n",
      "print \n",
      "print 'List of entity chunks'\n",
      "print nltk.batch_ne_chunk(pos_tags)[:1][0][:6]\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "List of tagged tokens\n",
        "[('After', 'IN'), ('a', 'DT'), ('woman', 'NN'), ('called', 'VBN'), ('Angela', 'NNP'), ('Jarrell', 'NNP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "List of entity chunks\n",
        "[('After', 'IN'), ('a', 'DT'), ('woman', 'NN'), ('called', 'VBN'), Tree('PERSON', [('Angela', 'NNP'), ('Jarrell', 'NNP')]), ('is', 'VBZ')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>This is what we're after:</p>\n",
      "<pre><code>Tree('PERSON', [('Angela', 'NNP'), ('Jarrell', 'NNP')])</code></pre>\n",
      "<p>NLTK recognizes \"Angela\" and \"Jarrell\" as names, though it fails to identify them as <em>one name</em> (\"Angela Jarrell\"). If you wanted to treat <code>First</code> and <code>Last</code> names as a single name, there are ways to tune your classifier. Depending on the behavior you're shooting for, there could be a few ways to do it, so look to the NLTK docs for specifics.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Pause for a second</h2>\n",
      "<p>So far we:</p>\n",
      "<ul>\n",
      "    <li>Took corpus of text and split it up into sentences using <code>sent_tokenize</code></li>\n",
      "    <li>Split up each tokenized sentence into word tokens using <code>word_tokenize</code></li>\n",
      "    <li>Tagged each part of speech using <code>pos_tag</code></li>\n",
      "    <li>Converted the tagged parts of speech tokens into entity chunks using <code>batch_ne_chunk</code></li>\n",
      "</ul>\n",
      "\n",
      "<p>Let's wrap this routine into reusable functions.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Helper to read `corpus` column without the other columns</h4>\n",
      "<p>Pretty simple. This'll let us read the top n non-null corpuses quickly.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper function to read in text corpuses only\n",
      "def read_texts(f='./episodes_and_recaps.txt', n_samples=5):\n",
      "    \"returns non-null text corpuses for the top n rows\"\n",
      "    df = pd.read_csv(f,sep='|')\n",
      "    df = df[df.corpus.notnull()]\n",
      "    corpuses = df.corpus.head(n_samples).tolist()\n",
      "    return corpuses"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Take in raw text. Output tagged entity chunks.</h4>\n",
      "<p>Just copied and pasted the lines we already wrote into one function. This will puts a corpus thru the 4 operations we did above text => sentences => words => parts of speech => entity chunks. </p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parts_of_speech(corpus):\n",
      "    \"returns named entity chunks in a given text\"\n",
      "    sentences = nltk.sent_tokenize(corpus)\n",
      "    tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
      "    pos_tags  = [nltk.pos_tag(sentence) for sentence in tokenized]\n",
      "    return nltk.batch_ne_chunk(pos_tags, binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Find all the unique named entities.</h4>\n",
      "<p>This one will extract named entities from the entity chunks.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_entities(chunks):\n",
      "    \"given list of tagged parts of speech, returns unique named entities\"\n",
      "\n",
      "    def traverse(tree):\n",
      "        \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
      "        entity_names = []\n",
      "    \n",
      "        if hasattr(tree, 'node') and tree.node:\n",
      "            if tree.node == 'NE':\n",
      "                entity_names.append(' '.join([child[0] for child in tree]))\n",
      "            else:\n",
      "                for child in tree:\n",
      "                    entity_names.extend(traverse(child))\n",
      "    \n",
      "        return entity_names\n",
      "    \n",
      "    named_entities = []\n",
      "    \n",
      "    for chunk in chunks:\n",
      "        entities = sorted(list(set([word for tree in chunk\n",
      "                            for word in traverse(tree)])))\n",
      "        for e in entities:\n",
      "            if e not in named_entities:\n",
      "                named_entities.append(e)\n",
      "    return named_entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Test it out</h4>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = read_texts(n_samples=1)[0][:500]\n",
      "print text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "David Moore, a wealthy Manhattanite, comes rushing into the lobby of his apartment building, carrying his comatose wife Joan in his arms. In the hospital, Joan Moore is diagnosed as suffering from insulin shock, even though she is not a diabetic. On further investigation, Green and Briscoe find out that Joan is actually suffering from Parkinson's disease\u2014and, unbeknownst to her husband, has been receiving treatment from Dr. Richard Shipman\u2014and suspect that her husband is either attempting to\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entity_chunks  = parts_of_speech(text)\n",
      "find_entities(entity_chunks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['David Moore',\n",
        " 'Joan Moore',\n",
        " 'Bertrand Stokes',\n",
        " 'Briscoe',\n",
        " 'Green',\n",
        " 'Joan',\n",
        " 'Parkinson',\n",
        " 'Richard',\n",
        " 'David',\n",
        " 'Debbie',\n",
        " 'MPTP',\n",
        " 'Shipman']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>This is cool. Now what?</h2>\n",
      "<p>Applications for text-based entity extraction are far ranging, from exploring product conversations on Facebook to uncovering terrorist plots in emails, to analyzing free-response answers in customer surveys. Once you've built and tuned a model for your particular use-case, you can use it to power your app or use it within your CRM via Yhat.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Wrap the code you already wrote in a class</h4>\n",
      "<p>Define a subclass of <code>yhat.BaseModel</code>. Implement <code>require</code>, <code>transform</code>, and <code>predict</code> as usual. Here we require NLTK but not numpy or pandas since Yhat will load those for us by default.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from yhat import Yhat, BaseModel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NamedEntityFindr(BaseModel):\n",
      "    def require(self):\n",
      "        import nltk\n",
      "\n",
      "    def transform(self, raw):\n",
      "        \"uses the parts_of_speech function we wrote earlier\"\n",
      "        rows = pd.Series(raw['data'])\n",
      "        rows = rows.apply(parts_of_speech)\n",
      "        return rows\n",
      "\n",
      "    def predict(self, chunked):\n",
      "        \"uses the find_entities function we wrote earlier\"\n",
      "        res = chunked.apply(find_entities).values\n",
      "        return {'entities': res.tolist()[0]} # returns a nice dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Create an instance of that class</h4>\n",
      "<p>One super helpful feature of Yhat is that you can use any helper/utility functions you've written within your class. If you've referenced any functions from other parts of your script like the two we wrote, <code>parts_of_speech</code> and <code>find_entities</code>, you can pass those to your classifier when you create it by using the <code>udfs</code> argument. UDF is short for user defined function. This lets you explicitly tell Yhat which functions you want to use in production.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = NamedEntityFindr(\n",
      "    udfs=[find_entities, parts_of_speech]\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>Test it out locally</h4>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {'data': [text]}\n",
      "print data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'data': [\"David Moore, a wealthy Manhattanite, comes rushing into the lobby of his apartment building, carrying his comatose wife Joan in his arms. In the hospital, Joan Moore is diagnosed as suffering from insulin shock, even though she is not a diabetic. On further investigation, Green and Briscoe find out that Joan is actually suffering from Parkinson's disease\\xe2\\x80\\x94and, unbeknownst to her husband, has been receiving treatment from Dr. Richard Shipman\\xe2\\x80\\x94and suspect that her husband is either attempting to kill her or helping her commit suicide.Trying to trace the source of the insulin, the detectives run into the psychiatrist Bertrand Stokes, who has previously smuggled that substance into the country. Stokes' wife, however, explains that the insulin is part of a sex game in which a number of men inject their wives with insulin, have intercourse with them, tape the entire event, and swap the tapes among each other. David Moore admits that this is indeed happening, but claims he kept it from the police in order to spare his wife the embarrassment. Because he is accused of murdering Joan, David loses custody of her (and her fortune) to Joan's daughter, Debbie Mann.On further investigation, it turns out that Joan's Parkinson's disease is not natural, but induced by the drug MPTP. Remnants of this drug are subsequently found in Debbie's office, and it turns out that her company is on the brink of bankruptcy and is only kept afloat by an infusion of Joan's money. Green and Briscoe find out that Debbie and Shipman, who previously did research on MPTP, know each other and get Debbie to confess that the two conspired to slowly poison Joan. However, Debbie claims she pulled out of the arrangement at the last minute and that Shipman administered the fatal dosage of MPTP on his own. With the threat of reviving Joan through the drug L-dopa, Shipman finally confesses that this scenario is indeed true.\"]}\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>On your local machine, you need to transform your data using the transform function you wrote before you can call predict. In production, Yhat will do the transformation for you.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Results on my local machine'\n",
      "transformed = clf.transform(data)\n",
      "results = clf.predict(transformed)\n",
      "print results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results on my local machine\n",
        "{'entities': ['David Moore', 'Joan Moore', 'Bertrand Stokes', 'Briscoe', 'Green', 'Joan', 'Parkinson', 'Richard', 'David', 'Debbie', 'MPTP', 'Shipman']}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Deploy to Yhat</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yh = Yhat(\"YOUR USERNAME\", \"YOUR API KEY\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print yh.upload(\"NamedEntityFindr\", clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "uploading... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done!\n",
        "{u'status': u'success', u'modelname': u'NamedEntityFindr', u'version': 1}\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[model for model in yh.show_models()['models'] if model['name'] == \"NamedEntityFindr\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "[{u'className': u'NamedEntityFindr',\n",
        "  u'name': u'NamedEntityFindr',\n",
        "  u'username': u'austin',\n",
        "  u'version': 1}]"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results_from_server = yh.raw_predict(\"NamedEntityFindr\", 1, data)\n",
      "results_from_server"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "{u'execution_time': 1.5051939487457275,\n",
        " u'model': u'NamedEntityFindr',\n",
        " u'prediction': {u'entities': [u'David Moore',\n",
        "   u'Joan Moore',\n",
        "   u'Bertrand Stokes',\n",
        "   u'Briscoe',\n",
        "   u'Green',\n",
        "   u'Joan',\n",
        "   u'Parkinson',\n",
        "   u'Richard',\n",
        "   u'David',\n",
        "   u'Debbie',\n",
        "   u'MPTP',\n",
        "   u'Shipman']},\n",
        " u'run_date': 1372907338,\n",
        " u'status': u'success',\n",
        " u'user': u'austin',\n",
        " u'version': 1}"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'sanity check.'\n",
      "print 'results all match => %s' \\\n",
      "    % np.all(np.array(results['entities']) == np.array(results_from_server['prediction']['entities']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sanity check.\n",
        "results all match => True\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Final Thoughts</h2>\n",
      "<ul>\n",
      "    <li><a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html\" title=\"Categorizing and Tagging Words - NLTK docs\" target=\"_blank\">Categorizing and Tagging Words with NLTK</a> (NLTK docs)</li>\n",
      "    <li><a href=\"http://pixelmonkey.org/pub/nlp-training/\" title=\"Just Enough NLP with Python\" target=\"_blank\">Just Enough NLP with Python</a> (slides)</li>\n",
      "    <li><a href=\"http://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Foundations%20of%20Statistical%20Natural%20Language%20Processing%20-%20Christopher%20D.%20Manning.pdf\" title=\"Foundations of Statistical Natural Language Processing by Christopher Manning &amp; Hinrich Schiitze\" target=\"_blank\">Foundations of Statistical Natural Language Processing</a> by Christopher Manning &amp; Hinrich Schiitze (PDF)</li>\n",
      "    <li><a href=\"http://www.slideshare.net/japerk/nltk-in-20-minutes\" title=\"NLTK in 20 minutes\" target=\"_blank\">http://www.slideshare.net/japerk/nltk-in-20-minutes</a> (Slideshare)</li>\n",
      "    <li><a href=\"http://jpr.sagepub.com/content/50/3/319.full.pdf\" title=\"Shifting sands : Explaining and predicting phase shifts by dissident organizations\" target=\"_blank\">Shifting sands : Explaining and predicting phase shifts by dissident organizations</a> (PDF)</li>\n",
      "    <li><a href=\"http://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf\" title=\"sentiment analysis and opinion mining\" target=\"_blank\">the Sentiment Analysis book</a></li>\n",
      "    <li><a href=\"http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip\" title=\"customer review data zipfile\" target=\"_blank\">Customer Review Dataset</a> (Covers 5 products, .zip file)</li>\n",
      "    <li><a href=\"http://www-nlp.stanford.edu/software/\" title=\"The Stanford Natural Language Processing Group\" target=\"_blank\">The Stanford Natural Language Processing Group</a></li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}